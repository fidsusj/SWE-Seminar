\chapter{Conclusion} \label{chap:conclusion}

This chapter presents first of all the most important insights from each of the individual chapters. For this, the conclusions from each topic are summarized. Then, the improved knowledge areas from the SWEBOK are listed and their use over the chapters is compared. In the end, a final conclusion over all the topics is presented.

\textbf{Chapter \ref{sec:topic_2}} presents different approaches to create acceptance tests that can be automatically executed with the tool FitNesse. Due to the few results in the literature search, it seems that this topic using the specific tool FitNesse is not popular in the research. The first presented approach \cite{el-attar} is aimed at larger projects and uses lots of artefacts in the process. The second presented approach \cite{longo} is aimed to smaller projects and is adjusted for the use of US-UIDs, a type of artefact that was invented by some of the authors of the article. Whilst the first approach should be executed by a business analyst, the second approach is designed to be executed by the customer and the developers combined.  Both presented approaches are highly dependent on the experience and skill of the person executing the approach. Most of the steps are done manually and require human judgement. Therefore, the approaches are only recommended if the required artefacts are already part of the engineering process and a person with experience with these artefacts takes part in the engineering process.

\textbf{Chapter \ref{sec:topic_3}} showcases approaches to automatically derive test scenarios from means of the specification area using transition systems. This improves traceability between the specification and implementation and allows for an easy validation of requirements. The test cases can be derived by traversing the transition system. The first approach \cite{ClementineNebut2006} focuses on system level test generation whilst the second presented approach \cite{NajlaRaza2007} focuses on use case level test generation. Both presented approaches do not generate a sufficient amount of robustness tests for fault detection and do not sufficiently cover data variations. However, the presented approaches are still recommended for the automatic generation of functional test scenarios.

\textbf{Chapter \ref{sec:topic_4}} presents approaches to test real-time requirements. The literature search showed that this topic is not popular in the research. One of the presented approaches \cite{Siegl2010} involves manually executed, intermediate steps. Therefore, this approach cannot be recommended in the presented form. The other approach \cite{Guan2015} executes all intermediate steps automatically and can be recommended for usage.

\textbf{Chapter \ref{sec:topic_5}} focuses on testing with a classification tree. This technique offers the possibility to reduce the number of test cases. The first presented approach \cite{Conrad} describes how requirements can be transformed into a classification tree for the input parameters of a system model. Another presented article \cite{Belli} shows that classification trees are, despite existing, more recent methods, still relevant.

\textbf{Chapter \ref{sec:topic_6}} is missing.
\newpage
In \textbf{Chapter \ref{sec:topic_7}} the topic \textit{testing with system models} is discussed. The literature search showed that the articles relevant to this topic are mostly published between 2000-2009. Both presented approaches use requirements traceability in their model-based test generation processes and provide a tool support to generate tests automatically. The first approach \cite{Paper1} provides test generation tool Qtronic, which is not existing anymore. Therefore, the second approach \cite{Paper2} could be recommended for the automatic test generation with system models, because it provides LEIRIOS Test Designer tool (currently named Smartesting), which is still available. However, both approaches lack the detailed explanation on how the test cases are generated via the provided test generation tools.

\textbf{Chapter \ref{sec:topic_8}} presents approaches to automatically generate test cases for functional and non-functional requirements from User Requirements Notation. The literature search showed that this topic has not been researched enough, as this test generation process requires an intermediate step. The first presented approach \cite{ArnoldCorriveauShi2010} addresses both functional and non-functional requirements, however, the provided Validation Framework is not available and without it, this approach cannot be used. Although the second presented approach \cite{BoucherMussbacher2017} generates test cases only for functional requirements, it is still recommendable as the required frameworks, jUCMNav and JUnit, are still available and the whole process can be done quickly without problems. 

\textbf{Chapter \ref{sec:topic_9}} focuses on testing non-functional requirements with risk analysis. Both presented approaches deal with non-functional requirements and mention risk analysis only as a side note. Although the first presented approach \cite{ZouPavlovski2008} describes how non-functional requirements can be defined and controlled, it does not specify a way to test them except for simulating the operating condition. The second presented approach \cite{Lagerstedt2014} leaves it to software architects to define non-functional requirements and uses only architectural non-functional requirements that exist as code conventions and other guidelines. The conclusion is that non-functional requirements with higher risks need to be paid more attention to by giving the tests higher priority.

\textbf{Chapter \ref{sec:topic_10}} presents approaches to harness aspect-oriented techniques for testing non-functional requirements. While the first presented approach \cite{Metsa} points out the lacking tool support and the need for developers to have a firm understanding of aspect-oriented programming pose possible challenges, the second presented approach \cite{Duclos} solves these problems (partially) by introducing ACRE. ACRE is a tool that parses DSL statements to generate test aspects automatically, which only supports four aspect types and is limited to C++ applications. The literature search showed that systematic testing using aspects is not in focus of current research anymore due to its implementation difficulty. Nevertheless, existing approaches and tools should be further refined to make use of the advantages of aspect-oriented testing, especially for non-functional requirements. 

The improved knowledge areas of the SWEBOK according to the synthesis matrices of the individual chapters are the following:\\
Knowledge areas from the field of \textit{Software Testing} were improved by every topic. Improvements in the field of \textit{Software Requirements} are part of every topic except the topic presented in chapter \ref{sec:topic_5}. The topics presented in chapter \ref{sec:topic_5}, \ref{sec:topic_8}, \ref{sec:topic_9} and \ref{sec:topic_10} specifically improved a knowledge area from the field of \textit{Software Maintenance}. The field of \textit{Software Engineering Models and Methods} is improved by the topics presented in chapter \ref{sec:topic_3}, \ref{sec:topic_4}, \ref{sec:topic_7} and \ref{sec:topic_8}. Only the approaches from the topic of chapter \ref{sec:topic_2} and one of the approaches from the topic of chapter \ref{sec:topic_5} are mentioned to help with \textit{Software Construction}.

Furthermore, while the topics presented in chapter \ref{sec:topic_7} and \ref{sec:topic_10} are not popular in current research, the topics presented in chapter \ref{sec:topic_2}, \ref{sec:topic_4} and \ref{sec:topic_8} are generally not popular in research. However, still two different approaches could be found and are described for each topic of this work. Most of the topics of this work turned out to have potential to be automated at least partly. According to the synthesis matrices of the individual chapters other than the topics presented in chapter \ref{sec:topic_5} and \ref{sec:topic_9}, every topic included at least one approach that uses some automation steps for the creation of tests. Still, most of the work has to be done manually. Only the topic presented in chapter \ref{sec:topic_3} includes an approach that is mostly automated.

In Summary, most of the presented approaches are recommended under appropriate circumstances. These circumstances include having persons with the needed skills as in chapter \ref{sec:topic_2} or having access to specific tools as in chapter \ref{sec:topic_7} and \ref{sec:topic_8}. It should be mentioned that even though these approaches seem useful, often certain steps are not explained in detail. In particular, this is the case for the test generation steps such as in chapter \ref{sec:topic_3}, \ref{sec:topic_7}, and \ref{sec:topic_9}. 
