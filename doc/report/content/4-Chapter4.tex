\chapter{Testing with Transition Systems}

% 15 Seiten maxmimal

\section{Introduction}

System tests are used to make sure that clients receive exactly the kind of software that they previously specified within the order contract submitted to the software vendor. Oftentimes what was specified and what was implemented in the end does not match entirely. The software vendor therefore faces traceability problems to track which requirements could be covered in which tests. How does one make sure that each atomic functional and robustness requirement specified is covered in the requirements' implementation?

To solve this, the process of formal definition of requirements and matching system tests must be brought closer together, testing should already b√© possible in early stages of development, to be precise during the specification phase already. To automatically derive test scenarios from means of the specification area transition systems are used. They help to generate test paths through possible combinations of fine-grained functional requirements. An equivalent approach can be used to derive robustness tests as well. In this process requirements can furthermore be tested on their consistency, correctness and integration and eventually can be refined further.

For this purpose the two approaches \cite{ClementineNebut2006} and \cite{NajlaRaza2007} were analyzed. While \cite{ClementineNebut2006} was given in advance by the advisors, \cite{NajlaRaza2007} was discovered through an extensive literature search shown in \autoref{literaturesearch}. Both apporaches will be explained in the following sections \ref{approachone} and \ref{approachtwo} and applied to the movie management software example. A comparison between the two approaches will be drawn using a synthesis matrix shown in \autoref{comparison}. The main results of testing with transitions systems will be summarized in \autoref{conclusion}. 

Please refer to the glossar in order to receive a common understanding of the following terms used in the sections below: contract, operation, test criterion, test objective, test scenario, use case, use case scenario.

\section{Literature Search} \label{literaturesearch}

The literature research was driven by the central research question: \glqq Which approaches for automatic generation of system tests exist that are using contract enriched use cases or other use case related means of the specification area within a transition system simulation model?\grqq 

The focus during this literature search was on finding a second approach to automatically generate system tests from means of the specification phase by exhaustively simulating a transition system to generate test paths similar to \cite{ClementineNebut2006}. But as \cite{ClementineNebut2006} is restricted on using contract enriched use cases and use case scenarios, the way how test objectives are derived was this time freely selectable to receive another new, but similar approach. The pre-search results were promising both on IEEE Xplore and ACM. Only some papers on ACM could not be accessed publicly. The amount of results was considered to be sufficient to cover all relevant scientific papers, which is why research was restricted on these two platforms. Furthermore two relevance criteria inspired by the central research question were defined:

\begin{itemize}
	\item Does the method described in the article generate system tests automatically from use cases or other use case related means of the specification area?
	\item Are test objectives generated using some kind of simulation model based on use case contracts (pre- and postconditions) or similar transition system approaches?
\end{itemize} 

As system tests can not exclusively be derived from means of the specification area, an article should restrict to generating system tests from use case related means of the specification area. To derive test objectives a transition system should be simulated exhaustively based on contract definitions (pre- and postconditions). 

Search was done using both snowballing and search term techniques. 140 papers were found referencing \cite{ClementineNebut2006} and 46 articles were referenced by \cite{ClementineNebut2006}. The results of the snowballing search can be found in \autoref{snowballing}. The backward snowballing approach was restricted on references stated in the \textit{Related Work} chapter as all other references relate to preceding work that serve as basis knowledge to realize the transition system approach. Additionally, most of the references where quite old since the original paper was published in 2006. Therefore not all papers could be found on IEEE Xplore or ACM. Why specific papers were considered not suitable or only partly suitable is documented in \cite{FelixHausberger2020}.

\begin{table}[h] 
	\centering
	\begin{small}
		\caption{Results of snowballing techniques}
		\label{snowballing}
		\setlength{\tabcolsep}{1em}
		\begin{tabular}{l|c|c|c}
			\hline
			& \textbf{Yes} & \textbf{Possibly} & \textbf{No} \\
			\hline
			\hline	
			Forward snowballing & 5 & 9 & 63 \\
			\hline
			Backward snowballing & 2 & 1 & 2 \\
			\hline
		\end{tabular}
	\end{small}
\end{table}

Seach-term based search was done using the following key terms: system tests, automatic generation, transition system, simulation model, use cases, contracts. Only papers published between 2006 and 2020 havint the search term "test" and "use case" in its publication title were evaluated. 

\begin{small}
	\centering
	\begin{longtable}[h]{c|c|p{0.2\textwidth}|p{0.2\textwidth}|c}
		\caption{Results of search-term based technique}\label{search-term}\setlength{\tabcolsep}{1em}\\    %%%%<===
		\hline
		\textbf{Source} & \textbf{Date} & \textbf{Search restrictions} & \textbf{Search query} & \textbf{\#Results} \\
		\hline
		\hline	
		IEEE Xplore & 2020-22-21 & "system tests" in document title; "automatic generation" in document title; "transition system" in full text \& metadata; "simulation model" in full text \& metadata; "use cases" in document title; "contracts" in full text \& metadata; & "system tests" AND "automatic generation" AND "transition system" AND "simulation model" AND "use cases" AND contracts & 0 \\
		\hline
		IEEE Xplore & 2020-22-21 & "system tests" in document title; "automatic generation" in abstract; "transition system" in full text \& metadata; "use cases" in document title; "contracts" in full text \& metadata; & "system tests" AND "automatic generation" AND "transition system" AND "use cases" AND contracts & 0 \\
		\hline
		IEEE Xplore & 2020-22-21 & "test" in document title; "transition system" in full text \& metadata; "use cases" in document title & "system tests" AND "transition system" AND "use cases" & 82 \\
		\hline
		\hline
		ACM & 2020-22-21 & "system tests" in title; "automatic generation" in title; "transition system" in full text; "simulation model" in full text; "use cases" in title; "contracts" in full text; & "system tests" AND "automatic generation" AND "transition system" AND "simulation model" AND "use cases" AND contracts & 0 \\
		\hline
		ACM & 2020-22-21 & "system tests" in title; "automatic generation" in abstract; "transition system" in full text; "use cases" in title; "contracts" in full text; & "system tests" AND "automatic generation" AND "transition system" AND "use cases" AND contracts & 0 \\
		\hline
		ACM & 2020-22-21 & "system tests" in title; "transition system" in full text; "use cases" in title & "system tests" AND "transition system" AND "use cases" & 0 \\
		\hline
		ACM & 2020-22-21 & "test" in title; "use case" in title & "system tests" AND "use cases" & 8 \\
		\hline
	\end{longtable}
\end{small}

From the resulting papers, only one was considered suitable as a potential second paper. After the search for potential articles to be evaluated was finished, a choice between eight remaining papers from the initial search had to be made:

\begin{itemize}
	\item System Testing using UML Models,
	\item An Automatic Tool for Generating Test Cases from the System's Requirements,
	\item Automated Test Case Generation from Use Case: A Model Based Approach,
	\item Requirements Document Based Test Scenario Generation for Web Application Scenario Testing,
	\item An Approach to Modeling and Testing Web Applications Based on Use Cases,
	\item Test cases generation from UML state diagrams,
	\item Requirements by Contracts allow Automated System Testing,
	\item An Automated Approach to System Testing Based on Scenarios and Operations Contracts.
\end{itemize}

The decision criteria are based on the different search terms above mentioned and the already defined criteria. Additionally, focus of the selected paper should lie on creating system tests for any generic application area, not just UI parts of an application.

The paper \textit{An Automatic Tool for Generating Test Cases from the System's Requirements} was not chosen as it does not focus on testing the consistency of use case combinations with contracts to build test objectives as in the original paper. Furthermore, it is not as in-depth as the original paper. Contract enriched use cases could neither be found in \textit{System Testing using UML Models}.

\textit{Automated Test Case Generation from Use Case: A Model Based Approach} really embodies the principle of state base modeling based on use cases with its \textit{interaction finite automaton} (IFA), but doesn't introduce a formal language to define use cases and its contracts.

\textit{Requirements Document Based Test Scenario Generation for Web Application Scenario Testing} as well as \textit{An Approach to Modeling and Testing Web Applications Based on Use Cases} are specifically optimized for web application test scenarios and therefore not as general and universally applicable as the original paper.

\textit{Test cases generation from UML state diagrams} and \textit{Requirements by Contracts allow Automated System Testing} could unfortunately not be accessed in full length in IEEE Xplore.

The chosen article to further evaluate is \textit{An Automated Approach to System Testing based on Scenarios and Operations Contracts}, as it introduces a second way to create system tests from use case scenarios as UML 2.0 models by enriching it with contracts and by transforming the formalized scenarios to a transition system to validate test objectives. Further information on the paper will be introduced in the mid-term presentation.

\section{Approach 1} \label{approachone}

\subsection{Description of Approach 1}

The paper addresses the traceability problems between high-level use cases and concrete test case execution as well as the lack of integration of formal specification methods with well-established development life cycles. It shows a method to automatically generate executable test case scenarios by making use of a transition system and therefore shifting the effort of test generation to the specification activity. In the first step, UML use cases get enhanced with contracts (pre- and postconditions). The contracts are made executable by writing them in the form of requirement-level logical expressions. Through exhaustive simulation a transition system is built, which serves as a model for all valid sequences of use cases. Relevant test objectives get extracted from the transition system by applying predefined coverage criteria. Subsequently in the second step, test scenarios get generated by replacing each use case in a test objective with a use case scenario that is compatible in terms of static contract matching. The process results in executable test scenarios that get evaluated using the statement coverage metric.

\subsection{Application of Approach 1}

In the first step the test objectives have to be derived. Therefore we define the use cases (can be thought of as main functions) and their contracts (\autoref{contracts1}) as requirement-level logical expressions. The contracts are used to infer the correct partial ordering of functionalities that the system should offer. Only the use cases that really impact the state of the transition system were specified for this example. The notation used is equal to the one proposed in the paper. 

\begin{lstlisting}[caption={Contracts attached to use cases},label={contracts1}]
	
	UC createMovie(m: movie)
	post createdMovie(m)
	
	UC createLinkedPerformer(p: performer, m: movie)
	pre createdMovie(m)
	post createdPerformer(p) and createdLink(p,m)
	
	UC rateMovie(m: movie)
	pre createdMovie(m)
	post calculatedOverallRating(m)
	
	UC ratePerformer(p: performer)
	pre createdPerformer(p)
	post forall(m: movie){ createdLink(p,m)@pre implies calculatedOverallRating(m) }
	
	UC linkExistingMovie(m: movie, p: performer)
	pre createdMovie(m) and createdPerformer(p)
	post not createdLink(p,m)@pre implies (createdLink(p,m) and calculatedOverallRating(m))
	
	UC linkExistingPerformer(m: movie, p: performer)
	pre createdMovie(m) and createdPerformer(p)
	post not createdLink(p,m)@pre implies (createdLink(p,m) and calculatedOverallRating(m))
	
	UC unlinkMovie(m: movie, p: performer)
	pre createdMovie(m) and createdPerformer(p) and createdLink(p,m)
	post calculatedOverallRating(m) and not createdLink(p,m) and not exists(m2: movie){ createdLink(p,m2) }@pre implies not createdPerformer(p)
	
	UC unlinkPerformer(m: movie, p: performer)
	pre createdMovie(m) and createdPerformer(p) and createdLink(p,m)
	post calculatedOverallRating(m) and not createdLink(p,m) and not exists(m2: movie){ createdLink(p,m2) }@pre implies not createdPerformer(p)
	
	UC removeMovie(m: movie)
	pre createdMovie(m)
	post not createdMovie(m) and forall(p: performer){ not createdLink(p,m) } and not exist(m2: movie){ createdLink(p,m2) }@pre implies not createdPerformer(p)
	
	UC removePerformer(p: performer)
	pre createdPerformer(p)
	post not createdPerformer(p) and forall(m: movie){ not createdLink(p,m) }
\end{lstlisting}

After that the UCSystem prototype/interpreter tool should build the UCTS (\autoref{ucts}) through exhaustive simulation. The pool of parameters was restricted to one movie and performer to avoid a combinatorical explosion for this example. To build instantiated use cases the set of formal parameters are replaces with all the possible combinations of their actual vales. In our case we use the most simple approach by just having one possible combination. Furthermore the predicate calculatedOverallRating is no longer considered. Note that only predicates that evaluate to true are listed in the states as in the original paper. 

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{./images/ucts.png}
	\caption{The Use Case Transition System}
	\label{ucts}
\end{figure}

After applying an instantiated use case in the transition system (in case the precondition of the contract was fulfilled) the simulation state is updated according to the contracts' postcondition. 

Depending on the selected coverage criterion, we receive different test objectives as correct sequences of use cases. The robustness criterion was not considered in this example, but its application is coherent to the functional coverage criterions. How many test objectives are derived depends on the internal implementation of UCSystem and cannot be predicted for this example. Let's assume that one test objective is the test path createMovie(m) -> createLinkedPerformer(p,m) -> removeMovie(m). Then the use case scenarios from \autoref{ucs} are used to replace the use cases in the test objectives. It helps to specify the exchange of messages involved between the environment and the system.

\newpage

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{./images/ucs.png}
	\caption{The Use Case Scenarios}
	\label{ucs}
\end{figure}

Note that the use case scenarios may still be incomplete for the execution. They contain the main messages exchanged between the tester and the SUT and say how the system has to be simulated to perform a use case and how to react to the simulation. To know how the system has to be simulated, the use case scenarios contain more detailed contracts written in OCL besides the contracts written as logical expressions that were provided by the use cases. 

The UC-SCSystem uses the shown implementation in the use case scenarios to derive executable test scenarios as JUnit tests.

\section{Approach 2} \label{approachtwo}

\subsection{Description of Approach 2}

Based on the suggested improvements in the first paper, the second paper uses interaction overview diagrams (IOD), a special form of activity diagram used to show control flow, to derive test paths. It helps to start testing in early stages of development. The IODs get enhanced with contracts written in the object constraint language (OCL) and the get transformed into a contracts transition system (CTS) which models all scenarios of the IOD. Through traversing the CTS test paths get derived. Key difference to the first paper is that test scenarios do not get generated on system level but rather on use case level due to the fact that contracts are not attached to use cases but to IODs. It therefore serves as a platform to generate more in-depth test scenarios (as well for negative test cases).

\subsection{Application of Approach 2}

The second approach differs from the first one as this time a transition system is built on a concrete use case, in our case the use case to unlink a movie from a performer. Input to the approach in this paper is the IOD (\autoref{iod}) with separate contracts defined in an OCL file (\autoref{contracts2}). IODs are a special form of activity diagrams used to show the control flow. The nodes in our case are UML sequence diagrams and define the operations of the contract transition system. The states are represented by the contracts themselves. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{./images/iod.png}
	\caption{Interaction Overview Diagram}
	\label{iod}
\end{figure}

\begin{lstlisting}[caption={Contracts written in OCL},label={contracts2}]
	context Movie::unlink(performer)
	pre  self.performers[performer] -> not isEmpty()
	post self.performers[performer] -> isEmpty()
	
	context MovieManager::removePerformer(performer)
	pre forAll(movie | movie.performers[performer] -> isEmpty())
	post self.performers[performer] -> isEmpty()
	
	context Movie::calculateOverallRating()
	post self.rating = 0.5 * (self.mean(self.performers.getRatings()) + self.rating)
\end{lstlisting}

\newpage

Based on the IOD and the specified contracts the CTS matrix gets defined and leads to the CTS shown in \autoref{cts}.

\begin{longtable}[h]{llll}
	Operations & Pre & Post & Composite States \\
	$O_{1}$ & $S_{0}$ & $S_{1}$ OR $S_{2}$ & A \\
	$O_{2}$ & $S_{1}$ & $S_{3}$ OR $S_{4}$ & B \\
	$O_{3}$ & $S_{3}$ OR $S_{4}$ & $S_{2}$ & \\
	$O_{4}$ & $S_{2}$ & $S_{5}$ & \\
\end{longtable}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{./images/cts.png}
	\caption{Contracts Transition System}
	\label{cts}
\end{figure}

Based on a coverage criterion the test paths get derived. Differing from the first approach no test scenarios get generated. The paper shows a new more low-level approach to generate test paths as this was even a suggested improvement from the authors of the first paper. 

\section{Comparison} \label{comparison}

\begin{enumerate}
	\item Description of the approach (What does the approach do?)
	\begin{enumerate}
		\item Which artifacts and relations between artifacts are used in this approach? Which artifacts are created in the course of the approach? How are the artifacts characterized?
		\item What is required and/or input for the application of the approach?
		\item What steps does the approach consist of? Which information is used in which step and how? What are the results of the individual steps?
	\end{enumerate}
	\item Benefits of the approach (Who does the approach help and how?)
	\begin{enumerate}
		\item Which usage scenarios are supported by the approach?
		\item Which stakeholders are supported by the usage scenarios?
		\item Which knowledge areas from SWEBOK can be assigned to the usage scenarios?
	\end{enumerate}
	\item Tool support for the approach (What tool support is available?)
	\begin{enumerate}
		\item What tool support is provided for the approach?
		\item Which steps of the approach are automated by a tool? Which steps are supported by a tool, but still have to be executed manually? Which steps are not supported by a tool?
	\end{enumerate}
	\item Quality of the approach (How well does the approach work?)
	\begin{enumerate}
		\item How was the approach evaluated?
		\item What are the (main) results of the evaluation?
	\end{enumerate}
\end{enumerate}

\begin{longtable}[h]{p{0.5cm}p{0.5\textwidth}p{0.5\textwidth}}
	Nr. & Approach \cite{ansatz.2006} & Approach \cite{ansatz.2007} \\
	3a & 
	Use cases describing the basic operations in the transition system. Contracts that are attached to the use cases describing the states in the transition system. The transition system (UCTS) itself as a simulation model to derive test objectives from. Test objectives describing the test paths. UCSystem as a third party tool to build the UCTS and to derive test objectives using coverage criteria. Use case scenarios to build test scenarios from test objectives. UC-SCSystem to generate executable test scenarios.  & 
	IODs holding all scenarios and operations of a use case. Operations can either be interaction uses or sequence diagrams inside the IOD. Contracts written in OCL that are attached to the operations describing the states in the transition system. The CTS describing the transition system. Test paths derived from the CTS using coverage criteria.  \\
	3b & 
	Use cases, contracts written as logical expressions, use case scenarios (sequence diagrams), initial system state, selected coverage criterion and additional use case scenario parameters & 
	IODs, contracts written in OCL, selected coverage criterion, possibly manual resolving of conflicts in the CTS matrix \\
	3c &
	To express the ordering constraints between use cases, each use case is attached by a contract. A contract consists of pre- and postconditions that specify the system properties to make a use case applicable and which properties are aquired by the system after its application. Parameters to contracts are actors and main concepts of the use case. To simulate the use cases, the set of formal parameters of the contracts are replaced with all possible combinations of their actual values. The use cases are then called \textit{instantiated}. To apply an instantiated use case the precondition of its contract must match with the current simulation state. Afterwards the simulation state is updated according to the postcondition of the use case. Exhaustively simulating the system results in the use case transition system (UTCS). To derive test objectives the transition system is traversed according to one of the predefined coverage criteria {All Edges} (AE), \textit{All Vertices} (AV), \textit{All Instantiated Use Cases} (AIUC), \textit{All Vertices and All Instantiated Use Cases} (AV-AIUC) or \textit{All Precondition Terms criterion} (APT). Use case scenarios contain the main messages exchanged between the tester and the SUT, they define how the system has to be simulated to perform a use case and how to react to the simulation. To build test scenarios a use case scenario can replace a use case at a certain stage of execution iff the state reached at this stage locally implies the precondition of the use case scenario. Executable test scenarios are genereted by the UC-SCSystem. &
	Each operation in the IOD was enriched with its own contract written in OCL. To build the CTS, first all operations have to be identified from the IOD. The operations are taken from the sequence diagrams or from other operations expressed as interaction occurences in the IOD. Using the contracts of operations, the states for the CTS are identified. Eventually conflicts in the CTS have to be resolved such as logical if-then-else conditions, equal contract statements or join nodes. After the CTS was built, test paths are derived from the CTS by applying a coverage criterion, which is either state-, transition- or transition pair coverage. \\
	4a & 
	Automatic test generation from use cases and use case scenarios. Requirement validation by identifying inconsistencies, underspecifications and invariants. & 
	Deriving test paths from IODs. Further requirement validation on use case level\\
	4b & 
	Test writers / Developers, Requirement Engineers &
	Test writers / Developers, Requirement Engineers \\
	4c &
	Software Requirements (definition of a software requirement, functional requirements, acceptance tests), Software Testing (model-based techniques, objectives of testing, evaluation of the tests performed), Software Engineering Models and Methods (preconditions, postconditions and invariants, behavioral modeling, analysis for consistency and correctness, traceability) &
	Software Requirements (definition of a software requirement, functional requirements, acceptance tests), Software Testing (model-based techniques), Software Engineering Models and Methods (preconditions, postconditions and invariants, behavioral modeling, analysis for consistency and correctness) \\
	5a & 
	Dedicated editor to design use cases with contracts, UCSystem to build the UCTS simulation model and to derive test objectives from it. UC-SCSystem to exchange the use cases by use case scenarios in order to build the executable test case scenarios & 
	UML 2.0 as a standard for IODs, OCL as formal language to write the contracts, prototype tool to derive test paths \\
	5b & 
	Writing the use cases and contracts is supported by a dedicated editor, but has to be done manually. Deriving test objectives from use cases and contracts through the transition system is done automatically by UCSystem. Use case scenarios have to be specified manually, the generation of test scenarios works semi-automatically with UC-SCSystem as it may need additional parameters from the tester. & 
	Only the IOD and contract specification has to be done manually, the complete approach was then automized by a prototype tool \\
	6a & 
	The approach was evaluated by looking at the statement coverage of three sample programs and the efficiency of test case scenario generation & 
	The approach was evaluated by looking at the number of test paths generated to cover all success scenarios and fault detections \\
	6b &
	Code coverage with the most coverage criteria was around 80\%. The Coverage criteria differ in efficiency. AE, AV and AV-AIUC perform with low efficiency, the sets of test cases are larger than in AIUC and APT. APT reaches 100\% functional test coverage with only 15 test case scenarios. Testing robustness leads to a high number of generated test case scenarios that only cover about 50\% of the corresponding code. The approach is good for functional testing, but bad for robustness testing. &
	Using the transition criterion to derive test paths leads to a reasonable amount of test paths and covers all alternative flows in the IOD, but is not suitable for fault detection at any time. The transition pair coverage criterion guarantees the maximum fault detection, but leads to a high amount of test paths. State coverage captures all success scenarios. \\
\end{longtable}
% Feedback einarbeiten

\section{Conclusion} \label{conclusion}

% Enth√§lt Aussagen zum Vergleich der Ans√§tze